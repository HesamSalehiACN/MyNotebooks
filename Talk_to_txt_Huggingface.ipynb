{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing the packages"
      ],
      "metadata": {
        "id": "Lf_Hgex6adhw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0UYcKiSFFPF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importing the packages"
      ],
      "metadata": {
        "id": "_H8uSYUPaml_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Enter Huggingface API Token\"\n",
        "from langchain.document_loaders import TextLoader  #for textfiles\n",
        "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
        "from langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "o4LA25gIFjXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pulling the txt file"
      ],
      "metadata": {
        "id": "peVZq1xfavDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/HesamSalehiACN/Documents/main/Trump_Speech.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"Trump_Speech.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "77o_RlDtHsxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Document loader"
      ],
      "metadata": {
        "id": "Nke1mLOZjzs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./Trump_Speech.txt')\n",
        "documents = loader.load()\n",
        "import textwrap\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "xkKJR42CHsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Splitting the text"
      ],
      "metadata": {
        "id": "fLyM551xj55F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "_nFRV_dJH4gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Embeddings"
      ],
      "metadata": {
        "id": "7sl_ovPkkAtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "YadC0h2nIEc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Creating a vectorstore"
      ],
      "metadata": {
        "id": "OBMI8SAVkKGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "WnCGtvLyIUo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Creating the chain/agent"
      ],
      "metadata": {
        "id": "92kjT0Tokc2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "# llm=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "# Alternative LLMs:\n",
        "llm=HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-783M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "gOof-nSlIZtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Query the data\n",
        "Remember that the the first run may be a little slow. If it takes more than 5 minutes means that most probably an error from the Hugging Face API will occur. Stop the run of the cell and try to run another one, and then run it again."
      ],
      "metadata": {
        "id": "G765x5Oekh2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"What is your question: \")\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "_21rTIeZI8QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -----------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "xz0J3prVrt3a"
      }
    }
  ]
}